{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from project_paths import paths \n",
    "from ipywidgets import interact, IntSlider\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "from lane_detection_medium.inference import DetectionInference\n",
    "from lane_detection_medium.evaluation import DetectionEvaluator\n",
    "from lane_detection_medium.utils.fs import get_date_string, read_image, read_yolo_labels\n",
    "from lane_detection_medium.utils.video_processing import VideoReader\n",
    "from lane_detection_medium.utils.viz import render_bbox\n",
    "from lane_detection_medium.types.detection_types import ImageDetections\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dpath = paths.yolo_dpath / \"data\" / \"2023_07_10\" / \"test\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"train-2023-07-10\"\n",
    "\n",
    "CHECKPOINT_DPATH = paths.yolo_dpath / \"LaneMarkingsDetection\" / EXP_NAME / \"weights\"\n",
    "MODEL_FPATH = CHECKPOINT_DPATH / \"best.pt\"\n",
    "\n",
    "inference = DetectionInference.from_file(\n",
    "    str(MODEL_FPATH), \n",
    "    device=\"cuda:0\", \n",
    "    img_size=(640, 640), \n",
    "    verbose=True,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dpath = paths.yolo_dpath / \"data\" / \"2023_07_10\" / \"test\"\n",
    "evaluator = DetectionEvaluator(model=inference, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_report = evaluator.evaluate(data_dpath)\n",
    "\n",
    "det_report"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Result Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dpath = paths.data / \"detection_results\" / f\"{EXP_NAME}_{get_date_string()}\"\n",
    "cache_dpath.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "det_report.to_csv(cache_dpath / \"metric_report.cvs\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_color_map = { \n",
    "    \"solid_white\": (255, 0, 0), \n",
    "    \"break_white\": (0, 0, 255), \n",
    "    \"zebra\": (255, 255, 0)\n",
    "}\n",
    "\n",
    "gt_color_map = { \n",
    "    \"solid_white\": (0, 255, 0), \n",
    "    \"break_white\": (255, 100, 100), \n",
    "    \"zebra\": (255, 255, 0)\n",
    "}\n",
    "\n",
    "img_fpaths = sorted(list((data_dpath / \"images\").glob(\"*.PNG\")))\n",
    "txt_fpaths = sorted(list((data_dpath / \"labels\").glob(\"*.txt\")))\n",
    "\n",
    "@interact \n",
    "def show_inference(index=IntSlider(val=0, min=0, max=len(img_fpaths) - 1)):\n",
    "    test_image = read_image(img_fpaths[index]) \n",
    "    gt_np = read_yolo_labels(txt_fpaths[index])\n",
    "    gt_dets = ImageDetections.from_yolo_labels(gt_np, *test_image.shape[:2])\n",
    "\n",
    "    detection_result = inference.detect([test_image], conf=0.25)[0]\n",
    "\n",
    "    canva = test_image.copy()\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "\n",
    "     # --- Ground Truth Rendering --- #\n",
    "    for gt_det in gt_dets:\n",
    "        label_name = inference.names_map[gt_det.label_id]\n",
    "        render_bbox(\n",
    "            canva, gt_det.bbox, label=label_name, color=gt_color_map[label_name], font_color=(255, 255, 255)\n",
    "        )\n",
    "\n",
    "    # --- Predicted results Rendering --- # \n",
    "    for det in detection_result:\n",
    "        label_name = f\"{det.label_name}: {det.conf:.2f}\"\n",
    "        render_bbox(\n",
    "            canva, det.bbox, label=label_name, color=pred_color_map[det.label_name], font_color=(255, 255, 255)\n",
    "        )\n",
    "\n",
    "    plt.title(img_fpaths[index].name)\n",
    "    plt.imshow(canva)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background Frames Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_fname = \"no_lines.mp4\"\n",
    "video_fpath = paths.data / \"videos\" / video_fname\n",
    "\n",
    "with VideoReader(video_fpath, verbose=True) as reader: \n",
    "    frames = list(reader.get_frames()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detections = inference.detect(frames, conf=0.25)\n",
    "len(detections)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "det_cnt = 0\n",
    "indices = [] \n",
    "for idx, det in enumerate(detections):\n",
    "    if len(det):\n",
    "        det_cnt += 1 \n",
    "        indices.append(idx)\n",
    "\n",
    "det_cnt, det_cnt / len(frames) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = { \n",
    "  \"solid_white\": (255, 0, 0), \n",
    "  \"break_white\": (0, 0, 255), \n",
    "  \"zebra\": (255, 255, 0)\n",
    "}\n",
    "\n",
    "@interact \n",
    "def show_inference(index=IntSlider(val=0, min=0, max=len(indices) - 1)):\n",
    "    data_idx = indices[index]\n",
    "    test_image = frames[data_idx]\n",
    "    detection_result = detections[data_idx]\n",
    "\n",
    "    canva = test_image.copy()\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "\n",
    "    for det in detection_result:\n",
    "      label_name = f\"{det.label_name}: {det.conf:.2f}\"\n",
    "      render_bbox(canva, det.bbox, label=label_name, color=color_map[det.label_name])\n",
    "\n",
    "    plt.imshow(canva)\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
