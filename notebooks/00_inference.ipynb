{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detection Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "from project_paths import paths \n",
    "from ipywidgets import interact, IntSlider\n",
    "\n",
    "from lane_detection_medium.inference import DetectionInference\n",
    "from lane_detection_medium.utils.fs import read_image\n",
    "from lane_detection_medium.utils.viz import render_bbox\n",
    "from lane_detection_medium.utils.video_processing import VideoReader, VideoWriter\n",
    "from lane_detection_medium.utils.fs import get_date_string\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trained Model Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXP_NAME = \"train-2023-07-05\"\n",
    "\n",
    "CHECKPOINT_DPATH = paths.yolo_dpath / \"LaneMarkingsDetection\" / EXP_NAME / \"weights\"\n",
    "MODEL_FPATH = CHECKPOINT_DPATH / \"best.pt\"\n",
    "\n",
    "inference = DetectionInference.from_file(\n",
    "    str(MODEL_FPATH), \n",
    "    device=\"cuda:0\", \n",
    "    img_size=(640, 640)\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "data_dpath = paths.yolo_dpath / \"data\" / \"2023_07_03\" / \"test\"\n",
    "img_fpaths = sorted(list((data_dpath / \"images\").glob(\"*.PNG\")))\n",
    "\n",
    "\n",
    "TEST_INDEX = 100 \n",
    "test_image = read_image(img_fpaths[TEST_INDEX])\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "plt.imshow(test_image)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Image Inference "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detection_result = inference.detect([test_image], conf=0.25)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "canva = test_image.copy()\n",
    "\n",
    "color_map = { \n",
    "  \"solid_white\": (255, 0, 0), \n",
    "  \"break_white\": (0, 0, 255), \n",
    "  \"zebra\": (255, 255, 0)\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "for det in detection_result:\n",
    "  label_name = f\"{det.label_name}: {det.conf:.2f}\"\n",
    "  render_bbox(canva, det.bbox, label=label_name, color=color_map[det.label_name])\n",
    "\n",
    "plt.imshow(canva)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Widget "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "color_map = { \n",
    "  \"solid_white\": (255, 0, 0), \n",
    "  \"break_white\": (0, 0, 255), \n",
    "  \"zebra\": (255, 255, 0)\n",
    "}\n",
    "\n",
    "@interact \n",
    "def show_inference(index=IntSlider(val=0, min=0, max=len(img_fpaths) - 1)):\n",
    "    test_image = read_image(img_fpaths[index]) \n",
    "    detection_result = inference.detect([test_image], conf=0.25)[0]\n",
    "\n",
    "    canva = test_image.copy()\n",
    "\n",
    "    plt.figure(figsize=(12, 12))\n",
    "\n",
    "    for det in detection_result:\n",
    "      label_name = f\"{det.label_name}: {det.conf:.2f}\"\n",
    "      render_bbox(canva, det.bbox, label=label_name, color=color_map[det.label_name])\n",
    "\n",
    "    plt.imshow(canva)\n",
    "    plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# video_fname = \"bad_road_example.mp4\" \n",
    "# video_fname = \"pulkovo.mp4\" \n",
    "video_fname = \"archangel1.mp4\" \n",
    "video_fpath = paths.data / \"videos\" / video_fname\n",
    "\n",
    "cache_dpath = paths.data / \"output_videos\" / get_date_string()\n",
    "cache_dpath.mkdir(parents=True, exist_ok=True)\n",
    "cache_fpath = cache_dpath / f\"output_{video_fname}\"\n",
    "\n",
    "color_map = { \n",
    "  \"solid_white\": (255, 0, 0), \n",
    "  \"break_white\": (0, 0, 255), \n",
    "  \"zebra\": (255, 255, 0), \n",
    "  \"solid_yellow\": (255, 0, 255), \n",
    "  \"break_yellow\": (0, 255, 255)\n",
    "}\n",
    "\n",
    "with VideoReader(video_fpath, verbose=True) as reader: \n",
    "    with VideoWriter(cache_fpath, fps=reader.fps) as writer: \n",
    "        for frame_img in reader.get_frames():\n",
    "            detections = inference.detect([frame_img], conf=0.25)[0]\n",
    "\n",
    "            canva = frame_img.copy()\n",
    "\n",
    "            for det in detections:\n",
    "                label_name = f\"{det.label_name}: {det.conf:.2f}\"\n",
    "                render_bbox(canva, det.bbox, label=label_name, color=color_map[det.label_name])\n",
    "            \n",
    "            writer.write_frame(canva)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
